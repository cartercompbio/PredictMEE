{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Entities<br>\n",
    "Adam Klie<br>\n",
    "11/02/2019<br>\n",
    "Script to predict entities in from trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Data visualization\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from string import punctuation\n",
    "\n",
    "# Neural nets\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to embed tokens from text into word embedding space\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = np.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import trained model, word embedding, and data to build validation data of off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iter = '11_class'\n",
    "model_date = '2020_07_09'\n",
    "grouping = pd.read_csv('../results/embedding/{model}/entity_merging.csv'.format(model=model_iter), index_col=0)\n",
    "groups = grouping[grouping[\"I\"] == 0][\"GroupName\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = np.load('../results/training/{model}/classes.npy'.format(model = model_iter))\n",
    "model = load_model('../models/{model}_{date}.h5'.format(model = model_iter, date=model_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/cellar/users/aklie/miniconda3/envs/deep_nlp_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('../data/wikipedia-pubmed-and-PMC-w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRS_dir = \"../data/allSRS_05_15_2018.pickle\"\n",
    "allSRS = pd.read_pickle(SRS_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict metadata for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_groups = iter(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through each class (run the following chunk for each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-97e37af2342f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidation_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_class = next(iter_groups)\n",
    "validation_class = validation_class.replace(' ', '_')\n",
    "validation_class = validation_class.replace('/', '_')\n",
    "validation_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in validation data for a specific class to predict on\n",
    "filename = '../results/validation/{model}/{myclass}_validation_set.pickle'.format(model = model_iter, \n",
    "                                                                                  myclass = validation_class)\n",
    "validation_data = pd.read_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking up into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "processed_test = validation_data.str.split('[;.,]', expand = True).stack()\n",
    "processed_test = processed_test.str.replace('\\s+', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Baseline emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict the empty state to use as baseline probability emission\n",
    "val_docs = list(nlp.pipe(' '))\n",
    "val_X = get_features(val_docs, max_length = model.input_shape[1])\n",
    "emptyState = model.predict_proba(val_X)[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "rows = []\n",
    "key_list = []\n",
    "for i, (key, sent) in enumerate(tqdm(processed_test.items(), total=len(processed_test))):\n",
    "    \n",
    "    # Sentence preprocessing\n",
    "    #sent = re.sub(r'[^a-zA-Z0-9]+', ' ', sent)  # remove non alpha numeric characters\n",
    "    tokens = re.split(pattern = ' ', string = sent)  # tokenize the description\n",
    "    tokens = list(filter(lambda token:(token!='') and (token not in stopWords), tokens))  # filter out stopwords\n",
    "    sent = ' '.join(tokens)\n",
    "    \n",
    "    n_gram_max = min([len(tokens), 7])\n",
    "    for n_gram in range(2, n_gram_max + 1):\n",
    "        \n",
    "        # Get prediction for all current n-grams\n",
    "        grams = list(map(lambda L:\" \".join(L), list(ngrams(tokens, n_gram))))  \n",
    "        val_docs = list(nlp.pipe(grams))  # get spacy objects for each token passed in\n",
    "        val_X = get_features(val_docs, max_length = model.input_shape[1])\n",
    "        predictM = model.predict_proba(val_X)\n",
    "        \n",
    "        # Take only those n-grams that have a total probability greater than the empty state + 0.01\n",
    "        # and also have two tokens present in word-embedding\n",
    "        tmp_df = pd.DataFrame(data = predictM, columns = le.classes_, index = grams)\n",
    "        empty_mask = (tmp_df - emptyState).abs().sum(axis=1) < 0.01\n",
    "        moreThanTwoValToken_mask = (val_X != 0).sum(axis=1) >= 2\n",
    "        tmp_df[empty_mask&moreThanTwoValToken_mask] = 0\n",
    "        \n",
    "        # Set up keys for dataframe with probabilities of each n-gram, will be useful later\n",
    "        for j, gram in enumerate(tmp_df.index):\n",
    "            i_end = j + n_gram\n",
    "            textBefore = \" \".join(tokens[:j]) + ('' if j==0 else ' ')\n",
    "            start_char_pos = len(textBefore)\n",
    "            key_list.append(key + (i, sent, n_gram, j, i_end, gram, start_char_pos)) \n",
    "            rows.append(tmp_df.iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "proba_df = pd.concat(rows, keys = key_list, axis = 1).T\n",
    "proba_df.index.names = ['srs', 'attribute', 'sentence_number', 'kthSrs', \n",
    "                        'orig_text', 'n-gram_length', 'word_start', 'word_end', 'token', \n",
    "                        'starting_char_pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Get only those greater than 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textS = pd.Series(proba_df.index.get_level_values('orig_text').unique())\n",
    "textM = textS.str.count(' ') >= 0\n",
    "selectedTexts = textS[textM].values # get the original texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_threshold = 2\n",
    "proba_sub = proba_df[(proba_df.index.get_level_values('n-gram_length') >= n_threshold) &\n",
    "                     (proba_df.index.get_level_values('orig_text').isin(selectedTexts))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Take highest probability class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_proba = proba_sub.max(axis=1)\n",
    "second_proba = proba_sub.quantile(0.999, interpolation='lower', axis = 1)\n",
    "scoreMargin_m = (max_proba-second_proba) > 0.1  # proba difference between 1st and 2nd must be greater than 0.1\n",
    "m_val = scoreMargin_m & (~proba_sub.index.get_level_values('token').str.contains('[0-9 ]+ [0-9 ]+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmpDf = pd.DataFrame({'predicted':proba_sub[m_val].idxmax(axis=1),'score':proba_sub[m_val].max(axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scoreSortedDf = tmpDf[m_val].sort_values(['orig_text','word_start','score'], ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Take highest probability overlapping n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "v = scoreSortedDf.copy()\n",
    "scoreSortedDf = scoreSortedDf.assign(OverlapGroup=(len(processed_test)*(v.kthSrs)+ \n",
    "                                          (v.word_end - v.word_start.shift(-1)).shift().lt(0).cumsum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hitDf=scoreSortedDf.sort_values(['OverlapGroup','score'],ascending=False).drop_duplicates(['OverlapGroup','predicted']\n",
    "                                                                                   ).sort_values('orig_text')\n",
    "hitDf['token_len']=hitDf['token'].str.len()\n",
    "hitDf['recovered_txt']=hitDf.apply(\n",
    "    lambda tmpS2:tmpS2.loc['orig_text'][tmpS2.loc['starting_char_pos']:(tmpS2.loc['starting_char_pos']+tmpS2.loc['token_len'])],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitDf.to_pickle('../results/prediction/{model}/{myclass}_prediction.pickle'.format(model = model_iter, \n",
    "                                                                                   myclass = validation_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
